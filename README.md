# üßÆ Mathematical Evaluation of LLM Behavior

This project explores mathematical approaches to evaluating large language models (LLMs), focusing on factuality, hallucination detection, and probabilistic modeling of reasoning. Methods include cosine similarity, entropy-based confidence, bootstrap sampling, and Markov chain analysis of multi-step reasoning. Experiments use HuggingFace Transformers and public benchmarks such as TruthfulQA, GSM8K, and AlpacaEval.

## üìÅ Tools & Libraries
- HuggingFace Transformers, Sentence-Transformers  
- PyTorch, NumPy, SciPy, scikit-learn  
- Matplotlib, Seaborn  
- Datasets: TruthfulQA, GSM8K, AlpacaEval  

## üîß Deliverables
- Modular Jupyter notebooks for metrics and modeling  
- Benchmark evaluation results and comparisons  
- Visualizations (ROC curves, entropy plots, error propagation diagrams)  
- Technical reports or blog-style write-ups  